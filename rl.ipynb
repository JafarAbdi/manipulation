{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.9 64-bit ('cherry-rl')",
      "metadata": {
        "interpreter": {
          "hash": "3ec851dc5512692319811246433c7896b8fdb84b490f8892b3918dcd9d4cac46"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    },
    "colab": {
      "name": "Robotic Manipulation - Let's get you a robot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgiF12Hf1Dhs",
        "colab_type": "text"
      },
      "source": [
        "**I recommend you run the first code cell of this notebook immediately, to start provisioning drake on the cloud machine, then you can leave this window open as you [read the textbook](manipulation.csail.mit.edu/rl.html).**\n",
        "\n",
        "# Notebook setup\n",
        "\n",
        "The following cell will:\n",
        "- on Colab (only), install Drake to `/opt/drake`, install Drake's prerequisites via `apt`, and add pydrake to `sys.path`.  This will take approximately two minutes on the first time it runs (to provision the machine), but should only need to reinstall once every 12 hours.  If you navigate between notebooks using Colab's \"File->Open\" menu, then you can avoid provisioning a separate machine for each notebook.\n",
        "- launch a server for our 3D visualizer (MeshCat) that will be used for the remainder of this notebook.\n",
        "\n",
        "You will need to rerun this cell if you restart the kernel, but it should be fast because the machine will already have drake installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeMrMI0-1Dhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import importlib\n",
        "import sys\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "if 'google.colab' in sys.modules and importlib.util.find_spec('pydrake') is None:\n",
        "  version='20200918'\n",
        "  build='nightly'\n",
        "  urlretrieve(f\"https://drake-packages.csail.mit.edu/drake/{build}/drake-{version}/setup_drake_colab.py\",\n",
        "              \"setup_drake_colab.py\")\n",
        "  from setup_drake_colab import setup_drake\n",
        "  setup_drake(version=version, build=build)\n",
        "  !pip install pyngrok==4.2.2\n",
        "  !pip install gym\n",
        "\n",
        "# Install pyngrok.\n",
        "server_args = []\n",
        "if 'google.colab' in sys.modules:\n",
        "  server_args = ['--ngrok_http_tunnel']\n",
        "\n",
        "# Start a single meshcat server instance to use for the remainder of this notebook.\n",
        "from meshcat.servers.zmqserver import start_zmq_server_as_subprocess\n",
        "#proc, zmq_url, web_url = start_zmq_server_as_subprocess(server_args=server_args)\n",
        "\n",
        "# Determine if this notebook is currently running as a notebook or a unit test.\n",
        "from IPython import get_ipython\n",
        "running_as_notebook = get_ipython() and hasattr(get_ipython(), 'kernel')\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "\n",
        "import pydrake.all"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tornado.web'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0d77fc825cff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Start a single meshcat server instance to use for the remainder of this notebook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmeshcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzmqserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstart_zmq_server_as_subprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#proc, zmq_url, web_url = start_zmq_server_as_subprocess(server_args=server_args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/drake-install/lib/python3.6/site-packages/meshcat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeometry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/drake-install/lib/python3.6/site-packages/meshcat/visualizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSetObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSetTransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDelete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSetProperty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSetAnimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeshPhongMaterial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mservers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzmqserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstart_zmq_server_as_subprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mViewerWindow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/drake-install/lib/python3.6/site-packages/meshcat/servers/zmqserver.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mADDRESS_IN_USE_ERROR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtornado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtornado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mioloop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtornado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebsocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tornado.web'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SjOClhTltPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "K, S = pydrake.systems.controllers.DiscreteTimeLinearQuadraticRegulator(A,B,Q,R)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'A' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f9e7099ceff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydrake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscreteTimeLinearQuadraticRegulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.78872546 -4.05613563]\n",
            "[-7.84486109 -1.16686604]\n",
            "[-9.01172713  0.15287092]\n",
            "[-8.85885621  0.10632339]\n",
            "[-8.75253282  1.33560422]\n",
            "[-7.41692859  2.93569866]\n",
            "[-4.48122993  3.25659222]\n",
            "[-1.22463771  3.41293831]\n",
            "[2.1883006  4.21202942]\n",
            "[6.40033002 3.19797154]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "from gym import spaces, logger\n",
        "from gym.utils import seeding\n",
        "\n",
        "class DoubleIntegrator(gym.Env):\n",
        "    metadata = {'render.modes': []}\n",
        "\n",
        "    def __init__(self):\n",
        "        # Discrete-time double \"integrator\"\n",
        "        self.A = np.array([[1, 1], [0, 1]])\n",
        "        self.B = np.array([0, 1])  # yuck.  \n",
        "\n",
        "        self.Q = np.identity(2)\n",
        "        self.R = np.identity(1)\n",
        "\n",
        "        self.state = None\n",
        "        self.action_space = spaces.Box(-np.inf, np.inf, shape=(1,))\n",
        "        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(2,))\n",
        "        self.seed()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "        u = action[0]\n",
        "        x = self.A.dot(self.state) + self.B*u\n",
        "        reward = -(x.dot(self.Q.dot(x)) + self.R*u*u)[0][0]\n",
        "        self.state = x\n",
        "        return np.array(x), reward, False, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.np_random.uniform(low=-5, high=5, size=(2,))\n",
        "        return np.array(self.state)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # intentionally blank\n",
        "        print(self.state)\n",
        "\n",
        "env = DoubleIntegrator()\n",
        "env.reset()\n",
        "for _ in range(10):\n",
        "    env.render()\n",
        "    env.step(env.action_space.sample()) # take a random action\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 0.00\n",
            "\n",
            "--------------------  Log 16 --------------------\n",
            "Overall:\n",
            "- Steps: 16000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 335395.65 +/- 0.00\n",
            "\n",
            "--------------------  Log 17 --------------------\n",
            "Overall:\n",
            "- Steps: 17000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 348986.32 +/- 0.00\n",
            "\n",
            "--------------------  Log 18 --------------------\n",
            "Overall:\n",
            "- Steps: 18000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 344701.76 +/- 0.00\n",
            "\n",
            "--------------------  Log 19 --------------------\n",
            "Overall:\n",
            "- Steps: 19000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 334008.53 +/- 0.00\n",
            "\n",
            "--------------------  Log 20 --------------------\n",
            "Overall:\n",
            "- Steps: 20000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 338885.44 +/- 0.00\n",
            "\n",
            "--------------------  Log 21 --------------------\n",
            "Overall:\n",
            "- Steps: 21000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 312817.59 +/- 0.00\n",
            "\n",
            "--------------------  Log 22 --------------------\n",
            "Overall:\n",
            "- Steps: 22000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 350863.83 +/- 0.00\n",
            "\n",
            "--------------------  Log 23 --------------------\n",
            "Overall:\n",
            "- Steps: 23000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 336965.57 +/- 0.00\n",
            "\n",
            "--------------------  Log 24 --------------------\n",
            "Overall:\n",
            "- Steps: 24000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 357221.39 +/- 0.00\n",
            "\n",
            "--------------------  Log 25 --------------------\n",
            "Overall:\n",
            "- Steps: 25000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 370876.77 +/- 0.00\n",
            "\n",
            "--------------------  Log 26 --------------------\n",
            "Overall:\n",
            "- Steps: 26000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 339025.47 +/- 0.00\n",
            "\n",
            "--------------------  Log 27 --------------------\n",
            "Overall:\n",
            "- Steps: 27000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 284011.29 +/- 0.00\n",
            "\n",
            "--------------------  Log 28 --------------------\n",
            "Overall:\n",
            "- Steps: 28000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 295710.39 +/- 0.00\n",
            "\n",
            "--------------------  Log 29 --------------------\n",
            "Overall:\n",
            "- Steps: 29000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 377921.98 +/- 0.00\n",
            "\n",
            "--------------------  Log 30 --------------------\n",
            "Overall:\n",
            "- Steps: 30000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 322754.77 +/- 0.00\n",
            "\n",
            "--------------------  Log 31 --------------------\n",
            "Overall:\n",
            "- Steps: 31000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 396029.15 +/- 0.00\n",
            "\n",
            "--------------------  Log 32 --------------------\n",
            "Overall:\n",
            "- Steps: 32000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 306702.03 +/- 0.00\n",
            "\n",
            "--------------------  Log 33 --------------------\n",
            "Overall:\n",
            "- Steps: 33000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 380036.53 +/- 0.00\n",
            "\n",
            "--------------------  Log 34 --------------------\n",
            "Overall:\n",
            "- Steps: 34000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 339463.78 +/- 0.00\n",
            "\n",
            "--------------------  Log 35 --------------------\n",
            "Overall:\n",
            "- Steps: 35000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 353991.05 +/- 0.00\n",
            "\n",
            "--------------------  Log 36 --------------------\n",
            "Overall:\n",
            "- Steps: 36000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 341038.55 +/- 0.00\n",
            "\n",
            "--------------------  Log 37 --------------------\n",
            "Overall:\n",
            "- Steps: 37000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 307227.95 +/- 0.00\n",
            "\n",
            "--------------------  Log 38 --------------------\n",
            "Overall:\n",
            "- Steps: 38000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 306944.74 +/- 0.00\n",
            "\n",
            "--------------------  Log 39 --------------------\n",
            "Overall:\n",
            "- Steps: 39000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 321295.22 +/- 0.00\n",
            "\n",
            "--------------------  Log 40 --------------------\n",
            "Overall:\n",
            "- Steps: 40000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 314036.97 +/- 0.00\n",
            "\n",
            "--------------------  Log 41 --------------------\n",
            "Overall:\n",
            "- Steps: 41000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 289653.42 +/- 0.00\n",
            "\n",
            "--------------------  Log 42 --------------------\n",
            "Overall:\n",
            "- Steps: 42000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 350992.04 +/- 0.00\n",
            "\n",
            "--------------------  Log 43 --------------------\n",
            "Overall:\n",
            "- Steps: 43000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 407952.40 +/- 0.00\n",
            "\n",
            "--------------------  Log 44 --------------------\n",
            "Overall:\n",
            "- Steps: 44000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 299710.60 +/- 0.00\n",
            "\n",
            "--------------------  Log 45 --------------------\n",
            "Overall:\n",
            "- Steps: 45000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 361405.54 +/- 0.00\n",
            "\n",
            "--------------------  Log 46 --------------------\n",
            "Overall:\n",
            "- Steps: 46000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 343706.87 +/- 0.00\n",
            "\n",
            "--------------------  Log 47 --------------------\n",
            "Overall:\n",
            "- Steps: 47000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 304569.11 +/- 0.00\n",
            "\n",
            "--------------------  Log 48 --------------------\n",
            "Overall:\n",
            "- Steps: 48000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 325563.84 +/- 0.00\n",
            "\n",
            "--------------------  Log 49 --------------------\n",
            "Overall:\n",
            "- Steps: 49000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 396339.17 +/- 0.00\n",
            "\n",
            "--------------------  Log 50 --------------------\n",
            "Overall:\n",
            "- Steps: 50000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 334988.81 +/- 0.00\n",
            "\n",
            "--------------------  Log 51 --------------------\n",
            "Overall:\n",
            "- Steps: 51000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 387320.93 +/- 0.00\n",
            "\n",
            "--------------------  Log 52 --------------------\n",
            "Overall:\n",
            "- Steps: 52000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 372717.97 +/- 0.00\n",
            "\n",
            "--------------------  Log 53 --------------------\n",
            "Overall:\n",
            "- Steps: 53000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 351711.47 +/- 0.00\n",
            "\n",
            "--------------------  Log 54 --------------------\n",
            "Overall:\n",
            "- Steps: 54000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 310020.44 +/- 0.00\n",
            "\n",
            "--------------------  Log 55 --------------------\n",
            "Overall:\n",
            "- Steps: 55000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 300989.86 +/- 0.00\n",
            "\n",
            "--------------------  Log 56 --------------------\n",
            "Overall:\n",
            "- Steps: 56000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 337292.11 +/- 0.00\n",
            "\n",
            "--------------------  Log 57 --------------------\n",
            "Overall:\n",
            "- Steps: 57000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 293657.60 +/- 0.00\n",
            "\n",
            "--------------------  Log 58 --------------------\n",
            "Overall:\n",
            "- Steps: 58000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 357860.05 +/- 0.00\n",
            "\n",
            "--------------------  Log 59 --------------------\n",
            "Overall:\n",
            "- Steps: 59000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 306783.03 +/- 0.00\n",
            "\n",
            "--------------------  Log 60 --------------------\n",
            "Overall:\n",
            "- Steps: 60000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 309239.31 +/- 0.00\n",
            "\n",
            "--------------------  Log 61 --------------------\n",
            "Overall:\n",
            "- Steps: 61000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 314996.51 +/- 0.00\n",
            "\n",
            "--------------------  Log 62 --------------------\n",
            "Overall:\n",
            "- Steps: 62000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 369709.95 +/- 0.00\n",
            "\n",
            "--------------------  Log 63 --------------------\n",
            "Overall:\n",
            "- Steps: 63000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 299685.97 +/- 0.00\n",
            "\n",
            "--------------------  Log 64 --------------------\n",
            "Overall:\n",
            "- Steps: 64000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 304499.00 +/- 0.00\n",
            "\n",
            "--------------------  Log 65 --------------------\n",
            "Overall:\n",
            "- Steps: 65000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 259606.49 +/- 0.00\n",
            "\n",
            "--------------------  Log 66 --------------------\n",
            "Overall:\n",
            "- Steps: 66000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 316231.76 +/- 0.00\n",
            "\n",
            "--------------------  Log 67 --------------------\n",
            "Overall:\n",
            "- Steps: 67000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 343516.14 +/- 0.00\n",
            "\n",
            "--------------------  Log 68 --------------------\n",
            "Overall:\n",
            "- Steps: 68000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 286642.82 +/- 0.00\n",
            "\n",
            "--------------------  Log 69 --------------------\n",
            "Overall:\n",
            "- Steps: 69000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 377526.07 +/- 0.00\n",
            "\n",
            "--------------------  Log 70 --------------------\n",
            "Overall:\n",
            "- Steps: 70000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 349274.15 +/- 0.00\n",
            "\n",
            "--------------------  Log 71 --------------------\n",
            "Overall:\n",
            "- Steps: 71000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 337909.63 +/- 0.00\n",
            "\n",
            "--------------------  Log 72 --------------------\n",
            "Overall:\n",
            "- Steps: 72000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 372923.08 +/- 0.00\n",
            "\n",
            "--------------------  Log 73 --------------------\n",
            "Overall:\n",
            "- Steps: 73000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 305180.57 +/- 0.00\n",
            "\n",
            "--------------------  Log 74 --------------------\n",
            "Overall:\n",
            "- Steps: 74000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 370301.00 +/- 0.00\n",
            "\n",
            "--------------------  Log 75 --------------------\n",
            "Overall:\n",
            "- Steps: 75000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 374652.26 +/- 0.00\n",
            "\n",
            "--------------------  Log 76 --------------------\n",
            "Overall:\n",
            "- Steps: 76000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 357430.87 +/- 0.00\n",
            "\n",
            "--------------------  Log 77 --------------------\n",
            "Overall:\n",
            "- Steps: 77000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 314097.30 +/- 0.00\n",
            "\n",
            "--------------------  Log 78 --------------------\n",
            "Overall:\n",
            "- Steps: 78000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 337142.25 +/- 0.00\n",
            "\n",
            "--------------------  Log 79 --------------------\n",
            "Overall:\n",
            "- Steps: 79000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 353636.18 +/- 0.00\n",
            "\n",
            "--------------------  Log 80 --------------------\n",
            "Overall:\n",
            "- Steps: 80000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 364849.67 +/- 0.00\n",
            "\n",
            "--------------------  Log 81 --------------------\n",
            "Overall:\n",
            "- Steps: 81000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 330869.33 +/- 0.00\n",
            "\n",
            "--------------------  Log 82 --------------------\n",
            "Overall:\n",
            "- Steps: 82000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 312616.82 +/- 0.00\n",
            "\n",
            "--------------------  Log 83 --------------------\n",
            "Overall:\n",
            "- Steps: 83000\n",
            "- Episodes: 0\n",
            "Last 10 Episodes:\n",
            "- Mean episode length: 0.00 +/- 0.00\n",
            "- Mean episode reward: 0.00 +/- 0.00\n",
            "Last 1000 Steps:\n",
            "- Episodes: 1\n",
            "- Mean episode length: 1000.00 +/- 0.00\n",
            "- Mean episode reward: 379680.17 +/- 0.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from itertools import count\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import cherry as ch\n",
        "import cherry.envs as envs\n",
        "\n",
        "SEED = 567\n",
        "GAMMA = 0.99\n",
        "RENDER = False\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "th.manual_seed(SEED)\n",
        "\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.affine1 = nn.Linear(2, 128)\n",
        "        self.affine2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.affine1(x))\n",
        "        return F.relu(self.affine2(x))\n",
        "\n",
        "\n",
        "def update(replay):\n",
        "    policy_loss = []\n",
        "\n",
        "    # Discount and normalize rewards\n",
        "    rewards = ch.discount(GAMMA, replay.reward(), replay.done())\n",
        "    rewards = ch.normalize(rewards)\n",
        "\n",
        "    # Compute loss\n",
        "    for sars, reward in zip(replay, rewards):\n",
        "        log_prob = sars.log_prob\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "\n",
        "    # Take optimization step\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = th.stack(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = DoubleIntegrator() #gym.make('CartPole-v0')\n",
        "    env = envs.Logger(env, interval=1000)\n",
        "    env = envs.Torch(env)\n",
        "    env.seed(SEED)\n",
        "\n",
        "    policy = PolicyNet()\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "    running_reward = 10.0\n",
        "    replay = ch.ExperienceReplay()\n",
        "\n",
        "    for i_episode in count(1):\n",
        "        state = env.reset()\n",
        "        for t in range(10000):  # Don't infinite loop while learning\n",
        "            density = Normal(policy(state), 0.1)\n",
        "            action = density.sample()\n",
        "            old_state = state\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            replay.append(old_state,\n",
        "                          action,\n",
        "                          reward,\n",
        "                          state,\n",
        "                          done,\n",
        "                          # Cache log_prob for later\n",
        "                          log_prob=density.log_prob(action))\n",
        "            if RENDER:\n",
        "                env.render()\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        #  Compute termination criterion\n",
        "        running_reward = running_reward * 0.99 + t * 0.01\n",
        "#        if running_reward > env.spec.reward_threshold:\n",
        "#            print('Solved! Running reward is now {} and '\n",
        "#                  'the last episode runs to {} time steps!'.format(running_reward, t))\n",
        "#            break\n",
        "\n",
        "        # Update policy\n",
        "        update(replay)\n",
        "        replay.empty()"
      ]
    },
    {
      "source": [
        "Garage pip version is 2020.06.3; i can browse that source [here](https://github.com/rlworkgroup/garage/blob/e67db24f1a744049d452a123dfaaf719fa62ab46)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n2020-11-11 22:30:46 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] [ppo_cartpole] [ppo_cartpole] [ppo_cartpole] Logging to /home/russt/manipulation/data/local/experiment/ppo_cartpole_3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ppo_cartpole() got multiple values for argument 'ctxt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-69ee0f73384c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mppo_cartpole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctxt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/garage/experiment/experiment.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mctxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ppo_cartpole() got multiple values for argument 'ctxt'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from garage import wrap_experiment\n",
        "from garage.envs import GarageEnv\n",
        "from garage.experiment import LocalRunner\n",
        "from garage.experiment.deterministic import set_seed\n",
        "from garage.torch.algos import PPO\n",
        "from garage.torch.policies import GaussianMLPPolicy\n",
        "from garage.torch.value_functions import GaussianMLPValueFunction\n",
        "\n",
        "@wrap_experiment\n",
        "def ppo_cartpole(ctxt=None, seed=1):\n",
        "    \"\"\"Train PPO with CartPole-v0 environment.\n",
        "    Args:\n",
        "        ctxt (garage.experiment.ExperimentContext): The experiment\n",
        "            configuration used by LocalRunner to create the snapshotter.\n",
        "        seed (int): Used to seed the random number generator to produce\n",
        "            determinism.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    env = GarageEnv(env_name='CartPole-v0')\n",
        "\n",
        "    runner = LocalRunner(snapshot_config=ctxt)\n",
        "\n",
        "    policy = GaussianMLPPolicy(env.spec,\n",
        "                               hidden_sizes=[64, 64],\n",
        "                               hidden_nonlinearity=torch.tanh,\n",
        "                               output_nonlinearity=None)\n",
        "\n",
        "    value_function = GaussianMLPValueFunction(env_spec=env.spec,\n",
        "                                              hidden_sizes=(32, 32),\n",
        "                                              hidden_nonlinearity=torch.tanh,\n",
        "                                              output_nonlinearity=None)\n",
        "\n",
        "    algo = PPO(env_spec=env.spec,\n",
        "               policy=policy,\n",
        "               value_function=value_function,\n",
        "               max_path_length=100,\n",
        "               discount=0.99,\n",
        "               center_adv=False)\n",
        "\n",
        "    runner.setup(algo, env)\n",
        "    runner.train(n_epochs=100, batch_size=10000)\n",
        "\n",
        "\n",
        "ppo_cartpole(ctxt=ctxt, seed=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Logging to /home/russt/manipulation/data/local/experiment/cma_es_cartpole_1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Logging to /home/russt/manipulation/data/local/experiment/cma_es_cartpole_1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Logging to /home/russt/manipulation/data/local/experiment/cma_es_cartpole_1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Logging to /home/russt/manipulation/data/local/experiment/cma_es_cartpole_1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Logging to /home/russt/manipulation/data/local/experiment/cma_es_cartpole_1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Logging to /home/russt/manipulation/data/local/experiment/cma_es_cartpole_1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Setting seed to 1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Setting seed to 1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Setting seed to 1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Setting seed to 1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Setting seed to 1\n2020-11-11 22:21:09 | [ppo_pendulum] [ppo_pendulum] [ppo_pendulum] [cma_es_cartpole] [ppo_cartpole] [cma_es_cartpole] Setting seed to 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Variable policy/CategoricalMLPModel/mlp/hidden_0/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6c12f19d38d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mcma_es_cartpole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/garage/experiment/experiment.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mctxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6c12f19d38d8>\u001b[0m in \u001b[0;36mcma_es_cartpole\u001b[0;34m(ctxt, seed)\u001b[0m\n\u001b[1;32m     32\u001b[0m         policy = CategoricalMLPPolicy(name='policy',\n\u001b[1;32m     33\u001b[0m                                       \u001b[0menv_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                       hidden_sizes=(32, 32))\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearFeatureBaseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/garage/tf/policies/categorical_mlp_policy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_spec, name, hidden_sizes, hidden_nonlinearity, hidden_w_init, hidden_b_init, output_nonlinearity, output_w_init, output_b_init, layer_normalization)\u001b[0m\n\u001b[1;32m     94\u001b[0m             name='CategoricalMLPModel')\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/garage/tf/policies/categorical_mlp_policy.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                                                    shape=(None, None,\n\u001b[1;32m    104\u001b[0m                                                           self._obs_dim))\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             self._f_prob = tf.compat.v1.get_default_session().make_callable(\n\u001b[1;32m    107\u001b[0m                 [\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/garage/tf/models/model.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, name, *inputs)\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 tf.compat.v1.get_default_session().run(\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/garage/tf/models/categorical_mlp_model.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, state_input, name)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_normalization_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_normalization_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/garage/tf/models/mlp_model.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     91\u001b[0m                    \u001b[0moutput_w_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_w_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                    \u001b[0moutput_b_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_b_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                    layer_normalization=self._layer_normalization)\n\u001b[0m",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/garage/tf/models/mlp.py\u001b[0m in \u001b[0;36mmlp\u001b[0;34m(input_var, output_dim, hidden_sizes, name, input_var2, concat_layer, hidden_nonlinearity, hidden_w_init, hidden_b_init, output_nonlinearity, output_w_init, output_b_init, layer_normalization)\u001b[0m\n\u001b[1;32m     86\u001b[0m                                               \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_w_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                                               \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_b_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                                               name='hidden_{}'.format(idx))\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer_normalization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0ml_hid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_hid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 187\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1699\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \"\"\"\n\u001b[0;32m-> 1701\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/keras/legacy_tf_layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2099\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m         trainable=True)\n\u001b[0m\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/keras/legacy_tf_layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    449\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1570\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1572\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1313\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    519\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/home/russt/.virtualenvs/garage/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;31m# ResourceVariables don't have an op associated with so no traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;31m# Throw away internal tf entries and only take a few lines. In some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable policy/CategoricalMLPModel/mlp/hidden_0/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"This is an example to train a task with CMA-ES.\n",
        "Here it runs CartPole-v1 environment with 100 epoches.\n",
        "Results:\n",
        "    AverageReturn: 100\n",
        "    RiseTime: epoch 38 (itr 760),\n",
        "              but regression is observed in the course of training.\n",
        "\"\"\"\n",
        "from garage import wrap_experiment\n",
        "from garage.envs import GarageEnv\n",
        "from garage.experiment import LocalTFRunner\n",
        "from garage.experiment.deterministic import set_seed\n",
        "from garage.np.algos import CMAES\n",
        "from garage.np.baselines import LinearFeatureBaseline\n",
        "from garage.sampler import OnPolicyVectorizedSampler\n",
        "from garage.tf.policies import CategoricalMLPPolicy\n",
        "\n",
        "\n",
        "@wrap_experiment\n",
        "def cma_es_cartpole(ctxt=None, seed=1):\n",
        "    \"\"\"Train CMA_ES with Cartpole-v1 environment.\n",
        "    Args:\n",
        "        ctxt (garage.experiment.ExperimentContext): The experiment\n",
        "            configuration used by LocalRunner to create the snapshotter.\n",
        "        seed (int): Used to seed the random number generator to produce\n",
        "            determinism.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    with LocalTFRunner(ctxt) as runner:\n",
        "        env = GarageEnv(env_name='CartPole-v1')\n",
        "\n",
        "        policy = CategoricalMLPPolicy(name='policy',\n",
        "                                      env_spec=env.spec,\n",
        "                                      hidden_sizes=(32, 32))\n",
        "        baseline = LinearFeatureBaseline(env_spec=env.spec)\n",
        "\n",
        "        n_samples = 20\n",
        "\n",
        "        algo = CMAES(env_spec=env.spec,\n",
        "                     policy=policy,\n",
        "                     baseline=baseline,\n",
        "                     max_path_length=100,\n",
        "                     n_samples=n_samples)\n",
        "\n",
        "        runner.setup(algo, env)#, sampler_cls=OnPolicyVectorizedSampler)\n",
        "        runner.train(n_epochs=100, batch_size=1000)\n",
        "\n",
        "\n",
        "cma_es_cartpole()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}